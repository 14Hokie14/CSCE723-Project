{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSCE723 Final Project Code\n",
    "\n",
    "# Author: Joshua White\n",
    "# Sources: \n",
    "# https://realpython.com/python-keras-text-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports:\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable setup:\n",
    "train_filepath = \"training_set_0.csv\"\n",
    "test_filepath = \"test_set_0.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pandas dataframes\n",
    "df_train = pd.read_csv(train_filepath)\n",
    "df_test = pd.read_csv(test_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The .values takes a df series and turns it into a numpy array. \n",
    "documents_train = df_train['processed_text'].values\n",
    "categories_train = df_train['category'].values\n",
    "\n",
    "documents_test = df_test['processed_text'].values\n",
    "categories_test = df_test['category'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the SciKit Learn vectorizer to turn the documents into a sparse matrix. \n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(documents_train)\n",
    "\n",
    "X_train = vectorizer.transform(documents_train)\n",
    "X_test = vectorizer.transform(documents_test)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets create a logistic regression model, fit it, and get a score. \n",
    "# Note: had to raise max_iter because the fit was not converging. \n",
    "#    Default max_iter is 100. \n",
    "classifier = LogisticRegression(max_iter = 500)\n",
    "classifier.fit(X_train, categories_train)\n",
    "score = classifier.score(X_test, categories_test)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have the baseline model above we need to modify the categories_train & categories_test\n",
    "#    to work with the keras Sequential model. Right now each entry is just the integer of the class \n",
    "#    when it should be a one hot vector. So lets encode the output variable to make it work with keras. \n",
    "# Source:\n",
    "# https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(categories_train)\n",
    "encoded_train = encoder.transform(categories_train)\n",
    "encoded_test = encoder.transform(categories_test)\n",
    "categorical_train = np_utils.to_categorical(encoded_train)\n",
    "categorical_test = np_utils.to_categorical(encoded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(48, input_dim=input_dim, activation='relu'))\n",
    "model.add(layers.Dense(12, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, categorical_train,\n",
    "                    epochs=7,\n",
    "                    verbose=False,\n",
    "                    validation_data=(X_test, categorical_test),\n",
    "                    batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_train, categorical_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, categorical_test)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use this function to graph the accuracy and loss for the training\n",
    "#    and test data based on the history callback (which is the output of \n",
    "#    the fit() method of keras).\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we created the neural networks using sparse matrix's of the vocabulary. Now lets use \n",
    "#    word embeddings in the neural net. The index 0 is reserved and is not assigned to any\n",
    "#    word. \n",
    "\n",
    "# More imports:\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(documents_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(documents_train)\n",
    "X_test = tokenizer.texts_to_sequences(documents_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "\n",
    "# Can print an example here\n",
    "#print(documents_train[2])\n",
    "#print(X_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This X_train created by tokenizer is different than the one created by CountVectorizer. The\n",
    "#    one created by CountVectorizer is a vector for each entry that is the length of the entire\n",
    "#    vocabulary, where as this X_train will be a vector equal to the length of each text, and the\n",
    "#    numbers in vectors of this new X_train correspond to word values from the dictionary \n",
    "#    tokenizer.word_index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now because the lengths of our new X_train is all different we need some way to normalize them.\n",
    "#    One way to solve this issue is to pad the vectors smaller than some max length with a zero. \n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen_train = len(max(documents_train).split(' '))\n",
    "maxlen_test = len(max(documents_test).split(' '))\n",
    "maxlen = max(maxlen_train, maxlen_test)\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point, our data is still hardcoded. We have not told Keras to learn a new \n",
    "#    embedding space through successive tasks. Now you can use the Embedding Layer of \n",
    "#    Keras which takes the previously calculated integers and maps them to a dense \n",
    "#    vector of the embedding. \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "\n",
    "# This will be the size of the dense vector (the word embedding) that we are creating.\n",
    "embedding_dim = 50\n",
    "\n",
    "# Now lets set up the model layers:\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=maxlen))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(48, activation='relu'))\n",
    "model.add(layers.Dense(12, activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model, print out the accuracy scores, and graph the results using the history. \n",
    "history = model.fit(X_train, categorical_train,\n",
    "                    epochs=30,\n",
    "                    verbose=False,\n",
    "                    validation_data=(X_test, categorical_test),\n",
    "                    batch_size=10)\n",
    "loss, accuracy = model.evaluate(X_train, categorical_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, categorical_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets try it again but add a pooling layer to the neural network. \n",
    "embedding_dim = 50\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=maxlen))\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(layers.Dense(48, activation='relu'))\n",
    "model.add(layers.Dense(12, activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model, print out the accuracy scores, and graph the results using the history. \n",
    "history = model.fit(X_train, categorical_train,\n",
    "                    epochs=50,\n",
    "                    verbose=False,\n",
    "                    validation_data=(X_test, categorical_test),\n",
    "                    batch_size=10)\n",
    "loss, accuracy = model.evaluate(X_train, categorical_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, categorical_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets use precomputed word embeddings in our neural net. \n",
    "import numpy as np\n",
    "\n",
    "# We will use this function to retrieve the embedding matrix for the words from our documents. \n",
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets actually create our matrix using the tokenizer we already set up. \n",
    "\n",
    "embedding_dim = 50\n",
    "\n",
    "embedding_matrix = create_embedding_matrix('glove.6B.100d.txt', tokenizer.word_index, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see how many of the embedding vectors are nonzero, which is how well the pretrained \n",
    "#    vocabulary covers our corpus vocabulary. \n",
    "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
    "nonzero_elements / vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will use the word embeddings in the \n",
    "embedding_dim = 50\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size,embedding_dim, \n",
    "                           weights=[embedding_matrix],\n",
    "                           input_length=maxlen,\n",
    "                           trainable=False))\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(layers.Dense(36, activation='relu'))\n",
    "model.add(layers.Dense(12, activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model, print out the accuracy scores, and graph the results using the history. \n",
    "history = model.fit(X_train, categorical_train,\n",
    "                    epochs=60,\n",
    "                    verbose=False,\n",
    "                    validation_data=(X_test, categorical_test),\n",
    "                    batch_size=10)\n",
    "loss, accuracy = model.evaluate(X_train, categorical_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, categorical_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be the size of the dense vector (the word embedding) that we are creating.\n",
    "embedding_dim = 50\n",
    "\n",
    "# Now lets set up the model layers:\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size,embedding_dim, \n",
    "                           weights=[embedding_matrix],\n",
    "                           input_length=maxlen,\n",
    "                           trainable=True))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(38, activation='relu'))\n",
    "model.add(layers.Dense(24, activation='sigmoid'))\n",
    "model.add(layers.Dense(12, activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model, print out the accuracy scores, and graph the results using the history. \n",
    "history = model.fit(X_train, categorical_train,\n",
    "                    epochs=20,\n",
    "                    verbose=False,\n",
    "                    validation_data=(X_test, categorical_test),\n",
    "                    batch_size=10)\n",
    "loss, accuracy = model.evaluate(X_train, categorical_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, categorical_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try adding a convolutional layer to our neural net and see how\n",
    "#    that affects the score. \n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "# Now lets set up the model layers:\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size,embedding_dim, input_length=maxlen))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(36, activation='relu'))\n",
    "#model.add(layers.Dense(24, activation='relu'))\n",
    "model.add(layers.Dense(12, activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model, print out the accuracy scores, and graph the results using the history. \n",
    "history = model.fit(X_train, categorical_train,\n",
    "                    epochs=20,\n",
    "                    verbose=False,\n",
    "                    validation_data=(X_test, categorical_test),\n",
    "                    batch_size=10)\n",
    "loss, accuracy = model.evaluate(X_train, categorical_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.6f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, categorical_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.6f}\".format(accuracy))\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df = pd.DataFrame(history.history) \n",
    "max(hist_df['val_accuracy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
